				                     *** README***=======================================================***Tidy Version of the Human Activity Recognition Smartphones Dataset Version .01***=======================================================This document describes the work done to tidy a subset of the data provided through the work of Jorge Reyes-Ortiz, David Anguita, Alessandro Ghio, and Luca Oneto at the Smartlab - Non Linear Complex Systems Laboratory at DITTEN - the Universita degli Studi di Genova in Genoa, Italy ***Background***The authors carried out experiments with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, they captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments were video-recorded to label the data manually. The obtained dataset was then randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% the test data. The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. ***The Datasets***The data was downloaded as a zip file from:[https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip](https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip) A full description is available at the site where the data was obtained:[http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+SmartphonesThe unzipped dataset included the following files:- README.txt- features_info.txt: shows information about the variables used on the feature vector- activity_labels.txt: Links the class labels with their human -readable names- features.txt: list of all features- train/X_train.txt: Training set- train/Y_train.txt:  - train/subject_train.txt: Each row identifies the subject who performed the activity for each window sample in the training set- test/X_test.txt: Test set- test/Y_test.txt: Test labels- test/subject_test.txt: Each row identifies the subject who performed the activity for each window sample in the test setI addition each set of test and training data included a file of Intertial Signals. These files were deemed to be irrelevant to the task set out in the assignment and therefore were not considered.***Assignment***The assignment required to submission of: 1) a tidy data set , 2) a link to a Github repository with the script for performing the analysis, and 3) a code book that describes the variables, the data, and any transformations or work performed to clean up the data called CodeBook.md. You should also include a README.md in the repo with your scripts. This repo explains how all of the scripts work and how they are connected.***Deliverables***One R script called run_analysis.R that does the following.    1) Merges the training and the test sets to create one data set.    2) Extracts only the measurements on the mean and standard deviation for each         measurement.    3) Uses descriptive activity names to name the activities in the data set    4) Appropriately labels the data set with descriptive variable names.    5) From the data set in step 4, creates a second, independent tidy data set with the         average of each variable for each activity and each subject.The assignment can be found at: [https://www.coursera.org/learn/data-cleaning/peer/FIZtT/getting-and-cleaning-data-course-project](https://www.coursera.org/learn/data-cleaning/peer/FIZtT/getting-and-cleaning-data-course-project)***Approach***Before starting to work on the data in R, considerable time was spent looking at the data in Notepad++ and Excel and reading the documentation provided. A link provided by Blanca Silva the Coursera Week 4 Discussion Group led to an article byDavid Hood *"Getting and Cleaning the Assignment" * ([*https://thoughtfulbloke.wordpress.com/2015/09/09/getting-and-cleaning-the-assignment/*](*https://thoughtfulbloke.wordpress.com/2015/09/09/getting-and-cleaning-the-assignment/*)) proved to be extremely helpful.By comparing the dimensions of each of the three files in in the test and train folders and examining the features.txt file, I concluded that:1) The core of the information was held in the X_test.txt and X_train.txt files2) the features.txt file contained the basis for meaningful column names for the core     data3) subject_train.txt and subject_test.txt files contained the data for a new column in    their corresponding X files to be named "subject"4) subject_train.txt and subject_test.txt files contained the data for another new    column their corresponding X files to be names "activity"5) The activity_labels.txt file could be used to map activity data in the newly formed activity column to meaningful activity names**Method***The following steps  created the run_analysis.R script:## Title: Run_analysis.R - creating a tidy version of a subset of the UCIHAR datasetsStep 1: Set library (dplyr) - required for the script to work correctlyStep2: read the following files as tables from the test folder:            - X_test, Y_test, subject_test              and use cbind to append them together with subject_test becoming Column             1,  Y_test column 2 and then columns from the X_test.txt file.             The resulting matrix has 2,947 observations over 563 variablesStep3: Repeat step 2 with the files in the train folder. the resulting matrix has 7.352           observations over 563 variablesStep 4: Use rbind to merge test and train files to create a matrix called *mergedata* of Step 5: Create column names using features.txt file:            Read file, select a subset and transpose the            subset so that row values are now column valuesStep 6: clean makecolnames data. Remove ")","(","-"," characters. Replace the             occurrence of "BodyBody" with "Body". I decided not to convert the column             names to lowercase as I found them easier to read with the capitals identifying            each component of the measure.Step 7: create name for the new column with user ID's and activities and append            them to the front of makecolnamesStep 8: Assign column names to the mergedata matrixStep 9: convert activity numbers to activity names by converting the            $activity column to factor and then assigning the factor levels to            the names in column 2 of the activity_labels fileStep 10: Create a vector of desired column names containing the values "mean",         "std", "user", "activity" anywhere in the column name and then          unselect columns with "angle" in the column name. 79 columns selected          Step 11: Use the vector created in Step 9 to create a subset of  the columns I want -               those containing "mean", ,"user", "std" or "activity" in   the              column name. Resulting dataset has 10,299 observations over 81 variables (79              selected feature labels + subject + activityStep 12: Write the resulting file "tidyUCIHAR,csv" and upload to GitHub***Attribution***[1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012